{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback,EvalCallback \n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training on connect_four_v3.\n",
      "Using cuda device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\gymnasium\\core.py:311: UserWarning: \u001b[33mWARN: env.action_masks to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.action_masks` for environment variables or `env.get_wrapper_attr('action_masks')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "Eval num_timesteps=500, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 500      |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "d:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:243: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New best mean reward!\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "Eval num_timesteps=1000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1000     |\n",
      "---------------------------------\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "Eval num_timesteps=1500, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 1500     |\n",
      "---------------------------------\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "[WARNING]: Illegal move made, game terminating with current player losing. \n",
      "obs['action_mask'] contains a mask of all legal moves that can be chosen.\n",
      "Eval num_timesteps=2000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 9.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 9        |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 2000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.9     |\n",
      "|    ep_rew_mean     | -1       |\n",
      "| time/              |          |\n",
      "|    fps             | 100      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 20       |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=2500, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "------------------------------------------\n",
      "| eval/                   |              |\n",
      "|    mean_ep_length       | 10           |\n",
      "|    mean_reward          | -1           |\n",
      "| time/                   |              |\n",
      "|    total_timesteps      | 2500         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074922782 |\n",
      "|    clip_fraction        | 0.0359       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.89        |\n",
      "|    explained_variance   | -1.05        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | -0.0123      |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.0153      |\n",
      "|    value_loss           | 0.0581       |\n",
      "------------------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3000     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=3500, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 3500     |\n",
      "---------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-1.00 +/- 0.00\n",
      "Episode length: 10.00 +/- 0.00\n",
      "---------------------------------\n",
      "| eval/              |          |\n",
      "|    mean_ep_length  | 10       |\n",
      "|    mean_reward     | -1       |\n",
      "| time/              |          |\n",
      "|    total_timesteps | 4000     |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.3     |\n",
      "|    ep_rew_mean     | -0.99    |\n",
      "| time/              |          |\n",
      "|    fps             | 86       |\n",
      "|    iterations      | 2        |\n",
      "|    time_elapsed    | 47       |\n",
      "|    total_timesteps | 4096     |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Uses Stable-Baselines3 to train agents in the Connect Four environment using invalid action masking.\n",
    "\n",
    "For information about invalid action masking in PettingZoo, see https://pettingzoo.farama.org/api/aec/#action-masking\n",
    "For more information about invalid action masking in SB3, see https://sb3-contrib.readthedocs.io/en/master/modules/ppo_mask.html\n",
    "\n",
    "Author: Elliot (https://github.com/elliottower)\n",
    "\"\"\"\n",
    "import glob\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sb3_contrib import MaskablePPO\n",
    "from sb3_contrib.common.maskable.policies import MaskableActorCriticPolicy\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "\n",
    "import pettingzoo.utils\n",
    "from pettingzoo.classic import connect_four_v3\n",
    "\n",
    "\n",
    "class SB3ActionMaskWrapper(pettingzoo.utils.BaseWrapper):\n",
    "    \"\"\"Wrapper to allow PettingZoo environments to be used with SB3 illegal action masking.\"\"\"\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        \"\"\"Gymnasium-like reset function which assigns obs/action spaces to be the same for each agent.\n",
    "\n",
    "        This is required as SB3 is designed for single-agent RL and doesn't expect obs/action spaces to be functions\n",
    "        \"\"\"\n",
    "        super().reset(seed, options)\n",
    "\n",
    "        # Strip the action mask out from the observation space\n",
    "        self.observation_space = super().observation_space(self.possible_agents[0])[\n",
    "            \"observation\"\n",
    "        ]\n",
    "        self.action_space = super().action_space(self.possible_agents[0])\n",
    "\n",
    "        # Return initial observation, info (PettingZoo AEC envs do not by default)\n",
    "        return self.observe(self.agent_selection), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Gymnasium-like step function, returning observation, reward, termination, truncation, info.\"\"\"\n",
    "        super().step(action)\n",
    "        return super().last()\n",
    "\n",
    "    def observe(self, agent):\n",
    "        \"\"\"Return only raw observation, removing action mask.\"\"\"\n",
    "        return super().observe(agent)[\"observation\"]\n",
    "\n",
    "    def action_mask(self):\n",
    "        \"\"\"Separate function used in order to access the action mask.\"\"\"\n",
    "        return super().observe(self.agent_selection)[\"action_mask\"]\n",
    "\n",
    "\n",
    "def mask_fn(env):\n",
    "    # Do whatever you'd like in this function to return the action mask\n",
    "    # for the current env. In this example, we assume the env has a\n",
    "    # helpful method we can rely on.\n",
    "    return env.action_mask()\n",
    "\n",
    "\n",
    "def train_action_mask(env_fn, steps=10_000, seed=0, **env_kwargs):\n",
    "    \"\"\"Train a single model to play as each agent in a zero-sum game environment using invalid action masking.\"\"\"\n",
    "    env = env_fn.env(**env_kwargs)\n",
    "\n",
    "    print(f\"Starting training on {str(env.metadata['name'])}.\")\n",
    "\n",
    "    # Custom wrapper to convert PettingZoo envs to work with SB3 action masking\n",
    "    env = SB3ActionMaskWrapper(env)\n",
    "\n",
    "    env.reset(seed=seed)  # Must call reset() in order to re-define the spaces\n",
    "\n",
    "    env = ActionMasker(env, mask_fn)  # Wrap to enable masking (SB3 function)\n",
    "    # MaskablePPO behaves the same as SB3's PPO unless the env is wrapped\n",
    "    # with ActionMasker. If the wrapper is detected, the masks are automatically\n",
    "    # retrieved and used when learning. Note that MaskablePPO does not accept\n",
    "    # a new action_mask_fn kwarg, as it did in an earlier draft.\n",
    "    eval_callback = EvalCallback(env,\n",
    "                                best_model_save_path=\"./best_model_ln/\",\n",
    "                                log_path=\"./best_logs_ln/\",\n",
    "                                eval_freq=500,\n",
    "                                render=True\n",
    "                                ) \n",
    "    model = MaskablePPO(MaskableActorCriticPolicy, env, verbose=1)\n",
    "    model.set_random_seed(seed)\n",
    "    model.learn(total_timesteps=steps,callback=[eval_callback])\n",
    "\n",
    "    model.save(f\"{env.unwrapped.metadata.get('name')}_{time.strftime('%Y%m%d-%H%M%S')}\")\n",
    "\n",
    "    print(\"Model has been saved.\")\n",
    "\n",
    "    print(f\"Finished training on {str(env.unwrapped.metadata['name'])}.\\n\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "def eval_action_mask(env_fn, num_games=100, render_mode=None, **env_kwargs):\n",
    "    # Evaluate a trained agent vs a random agent\n",
    "    env = env_fn.env(render_mode=render_mode, **env_kwargs)\n",
    "\n",
    "    print(\n",
    "        f\"Starting evaluation vs a random agent. Trained agent will play as {env.possible_agents[1]}.\"\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        latest_policy = max(\n",
    "            glob.glob(f\"{env.metadata['name']}*.zip\"), key=os.path.getctime\n",
    "        )\n",
    "    except ValueError:\n",
    "        print(\"Policy not found.\")\n",
    "        exit(0)\n",
    "\n",
    "    model = MaskablePPO.load(latest_policy)\n",
    "\n",
    "    scores = {agent: 0 for agent in env.possible_agents}\n",
    "    total_rewards = {agent: 0 for agent in env.possible_agents}\n",
    "    round_rewards = []\n",
    "\n",
    "    for i in range(num_games):\n",
    "        env.reset(seed=i)\n",
    "        env.action_space(env.possible_agents[0]).seed(i)\n",
    "\n",
    "        for agent in env.agent_iter():\n",
    "            obs, reward, termination, truncation, info = env.last()\n",
    "\n",
    "            # Separate observation and action mask\n",
    "            observation, action_mask = obs.values()\n",
    "\n",
    "            if termination or truncation:\n",
    "                # If there is a winner, keep track, otherwise don't change the scores (tie)\n",
    "                if (\n",
    "                    env.rewards[env.possible_agents[0]]\n",
    "                    != env.rewards[env.possible_agents[1]]\n",
    "                ):\n",
    "                    winner = max(env.rewards, key=env.rewards.get)\n",
    "                    scores[winner] += env.rewards[\n",
    "                        winner\n",
    "                    ]  # only tracks the largest reward (winner of game)\n",
    "                # Also track negative and positive rewards (penalizes illegal moves)\n",
    "                for a in env.possible_agents:\n",
    "                    total_rewards[a] += env.rewards[a]\n",
    "                # List of rewards by round, for reference\n",
    "                round_rewards.append(env.rewards)\n",
    "                break\n",
    "            else:\n",
    "                if agent == env.possible_agents[0]:\n",
    "                    act = env.action_space(agent).sample(action_mask)\n",
    "                else:\n",
    "                    # Note: PettingZoo expects integer actions # TODO: change chess to cast actions to type int?\n",
    "                    act = int(\n",
    "                        model.predict(\n",
    "                            observation, action_masks=action_mask, deterministic=True\n",
    "                        )[0]\n",
    "                    )\n",
    "            env.step(act)\n",
    "    env.close()\n",
    "\n",
    "    # Avoid dividing by zero\n",
    "    if sum(scores.values()) == 0:\n",
    "        winrate = 0\n",
    "    else:\n",
    "        winrate = scores[env.possible_agents[1]] / sum(scores.values())\n",
    "    print(\"Rewards by round: \", round_rewards)\n",
    "    print(\"Total rewards (incl. negative rewards): \", total_rewards)\n",
    "    print(\"Winrate: \", winrate)\n",
    "    print(\"Final scores: \", scores)\n",
    "    return round_rewards, total_rewards, winrate, scores\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env_fn = connect_four_v3\n",
    "\n",
    "    env_kwargs = {}\n",
    "\n",
    "    # Evaluation/training hyperparameter notes:\n",
    "    # 10k steps: Winrate:  0.76, loss order of 1e-03\n",
    "    # 20k steps: Winrate:  0.86, loss order of 1e-04\n",
    "    # 40k steps: Winrate:  0.86, loss order of 7e-06\n",
    "\n",
    "    # Train a model against itself (takes ~20 seconds on a laptop CPU)\n",
    "    train_action_mask(env_fn, steps=20_480, seed=0, **env_kwargs)\n",
    "\n",
    "    # Evaluate 100 games against a random agent (winrate should be ~80%)\n",
    "    eval_action_mask(env_fn, num_games=100, render_mode='human', **env_kwargs)\n",
    "\n",
    "    # Watch two games vs a random agent\n",
    "    eval_action_mask(env_fn, num_games=2, render_mode=\"human\", **env_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cp810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
