{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "import gymnasium as gym\n",
    "from gymnasium.envs.registration import register\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "  save_freq=3125,\n",
    "  save_path=\"./checkpoint/\",\n",
    "  name_prefix=\"ppo_FIR\"\n",
    ")\n",
    "register(\n",
    "    id=\"five_in_roll-v0\",\n",
    "    entry_point=\"fir_env:FiveInRowEnv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_env(gym.make(\"five_in_roll-v0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "env = DummyVecEnv([lambda :Monitor(gym.make(\"five_in_roll-v0\")) for _ in range(10)])\n",
    "model = PPO(\n",
    "    \"CnnPolicy\", \n",
    "    env,\n",
    "    device=\"cuda\", \n",
    "    verbose=1,\n",
    "    n_steps=512,\n",
    "    batch_size=512,\n",
    "    n_epochs=4,\n",
    "    gamma=0.94,\n",
    "    learning_rate=5e-4,\n",
    "    tensorboard_log=\"logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to logs\\PPO_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/           |           |\n",
      "|    ep_len_mean     | 118       |\n",
      "|    ep_rew_mean     | -2.55e+06 |\n",
      "| time/              |           |\n",
      "|    fps             | 22        |\n",
      "|    iterations      | 1         |\n",
      "|    time_elapsed    | 230       |\n",
      "|    total_timesteps | 5120      |\n",
      "----------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 128           |\n",
      "|    ep_rew_mean          | -2.93e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 23            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 433           |\n",
      "|    total_timesteps      | 10240         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00022054338 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -42.6         |\n",
      "|    explained_variance   | 0             |\n",
      "|    learning_rate        | 0.0005        |\n",
      "|    loss                 | 2.5e+10       |\n",
      "|    n_updates            | 4             |\n",
      "|    policy_gradient_loss | -2.76e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.9e+10       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\项目\\python\\项目\\2023深度学习实训\\强化学习五子棋\\train.ipynb 单元格 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/%E9%A1%B9%E7%9B%AE/python/%E9%A1%B9%E7%9B%AE/2023%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%AE%AD/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%BA%94%E5%AD%90%E6%A3%8B/train.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m2\u001b[39;49m,callback\u001b[39m=\u001b[39;49m[checkpoint_callback])\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[39mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[0;32m    316\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[0;32m    317\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[0;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[0;32m    319\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[0;32m    320\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[0;32m    321\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    274\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 277\u001b[0m     continue_training \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, callback, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrollout_buffer, n_rollout_steps\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_steps)\n\u001b[0;32m    279\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m continue_training:\n\u001b[0;32m    280\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:194\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    190\u001b[0m         \u001b[39m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    191\u001b[0m         \u001b[39m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         clipped_actions \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(actions, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mlow, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mhigh)\n\u001b[1;32m--> 194\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(clipped_actions)\n\u001b[0;32m    196\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[0;32m    198\u001b[0m \u001b[39m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[39m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[0;32m     59\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[0;32m     60\u001b[0m         )\n\u001b[0;32m     61\u001b[0m         \u001b[39m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx] \u001b[39m=\u001b[39m terminated \u001b[39mor\u001b[39;00m truncated\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(\u001b[39mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[39mif\u001b[39;00m terminated \u001b[39mor\u001b[39;00m truncated:\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\gymnasium\\wrappers\\order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset:\n\u001b[0;32m     55\u001b[0m     \u001b[39mraise\u001b[39;00m ResetNeeded(\u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling env.reset()\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 56\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\Cp810\\lib\\site-packages\\gymnasium\\wrappers\\env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[39mreturn\u001b[39;00m env_step_passive_checker(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv, action)\n\u001b[0;32m     50\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n",
      "File \u001b[1;32md:\\项目\\python\\项目\\2023深度学习实训\\强化学习五子棋\\fir.py:167\u001b[0m, in \u001b[0;36mFiveInRowEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    165\u001b[0m     is_valide \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdown_white_chess(x,y)\n\u001b[0;32m    166\u001b[0m obs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_obs()\n\u001b[1;32m--> 167\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_reward(\u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnow_player) \u001b[39mif\u001b[39;00m is_valide \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39m60000\u001b[39m\n\u001b[0;32m    168\u001b[0m done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_done()\n\u001b[0;32m    169\u001b[0m Terminated \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[1;32md:\\项目\\python\\项目\\2023深度学习实训\\强化学习五子棋\\fir.py:132\u001b[0m, in \u001b[0;36mFiveInRowEnv.__get_reward\u001b[1;34m(self, nwp)\u001b[0m\n\u001b[0;32m    130\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__get_reward\u001b[39m(\u001b[39mself\u001b[39m,nwp):\n\u001b[0;32m    131\u001b[0m     ct \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m nwp \u001b[39melse\u001b[39;00m \u001b[39m2\u001b[39m\n\u001b[1;32m--> 132\u001b[0m     reward \u001b[39m=\u001b[39m check_reward(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchess_board[\u001b[39m0\u001b[39;49m, :, :],ct)\n\u001b[0;32m    133\u001b[0m     \u001b[39mreturn\u001b[39;00m reward\n",
      "File \u001b[1;32md:\\项目\\python\\项目\\2023深度学习实训\\强化学习五子棋\\check_reward.py:57\u001b[0m, in \u001b[0;36mcheck_reward\u001b[1;34m(board, ct)\u001b[0m\n\u001b[0;32m     55\u001b[0m rowScore \u001b[39m=\u001b[39m get_scores(rowV,rating_table,ct\u001b[39m=\u001b[39mct)\n\u001b[0;32m     56\u001b[0m colScore \u001b[39m=\u001b[39m get_scores(colV,rating_table,ct\u001b[39m=\u001b[39mct)\n\u001b[1;32m---> 57\u001b[0m digMScore \u001b[39m=\u001b[39m get_scores(diagMain,rating_table,ct\u001b[39m=\u001b[39;49mct)\n\u001b[0;32m     58\u001b[0m digVScore \u001b[39m=\u001b[39m get_scores(diagVice,rating_table,ct\u001b[39m=\u001b[39mct)\n\u001b[0;32m     59\u001b[0m \u001b[39mreturn\u001b[39;00m rowScore\u001b[39m+\u001b[39mcolScore\u001b[39m+\u001b[39mdigMScore\u001b[39m+\u001b[39mdigVScore\n",
      "File \u001b[1;32md:\\项目\\python\\项目\\2023深度学习实训\\强化学习五子棋\\check_reward.py:18\u001b[0m, in \u001b[0;36mget_scores\u001b[1;34m(vecs, rating_table, ct)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(vec[i:\u001b[39mlen\u001b[39m(pMask)\u001b[39m+\u001b[39mi]\u001b[39m==\u001b[39mpMask):\n\u001b[0;32m     17\u001b[0m             admx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(admx,scores)\n\u001b[1;32m---> 18\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mall\u001b[39m(vec[i:\u001b[39mlen\u001b[39;49m(tMask)\u001b[39m+\u001b[39;49mi]\u001b[39m==\u001b[39;49mtMask):\n\u001b[0;32m     19\u001b[0m             dfmx \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(dfmx,scores)\n\u001b[0;32m     20\u001b[0m df_scores\u001b[39m-\u001b[39m\u001b[39m=\u001b[39mdfmx\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=1e6//2,callback=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./model/FIR.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.load('./checkpoint/ppo_FIR_12500_steps.zip')\n",
    "for i in range(512):\n",
    "    state,info = env.reset()\n",
    "    done = False \n",
    "    score = 0\n",
    "    while not done:\n",
    "        action, _ = model.predict(observation=state)\n",
    "        obs,reward,done,Terminated,info = env.step(action)\n",
    "        score += reward\n",
    "        env.render()\n",
    "        if done:break\n",
    "        input()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Cp810",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
